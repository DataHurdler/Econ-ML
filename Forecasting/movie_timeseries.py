# -*- coding: utf-8 -*-
"""Movie_TimeSeries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ufhhpase57Ic8jfsdK2YI4NFkR6DUjRg

#About this notebook

* **Dataset**: Movie audience of Shanghai, from two sources, 2012-2015. It is reduced from the dataset used in my research paper *The Impact of Air Pollution on Movie Theater Admissions* (open access: https://www.sciencedirect.com/science/article/pii/S0095069622000134)
* **Data Cleaning**: I created a dataset that contains observations from Beijing, Tianjin, and Shanghai. Also contained in the dataset are daily temperature and precipitation. Each row represents a movie. In the first part of the notebook, I aggregate data into a univariate time series data using ```pandas```.
* **Analysis**: I employ two methods to analyze the time series data:
  1. *Exponential Smoothing* from ```statsmodels.tsa.HoltWinters```:
    * Model 1: Additive annual (365 days) seasonality but no trend
    * Model 2: Additive annual seasonality with additive trend
  2. Facebook's ```prophet```
* **Findings**:
  1. The two *Exponential Smoothing* models give similar results. Using the last 200 days as a **test set**, I calculate RMSE and MAE of the two models. RMSE shows that Model 1 performs better in the training set but worse in the test set. But MAE shows the opposite. Since the dataset has some abonormally high values, RMSE is the more appropriate metric.

  2. The ```prophet``` model does a good job in picking up not only annual seasonality but also weekly seasonality. Weekly seasonality can be important in this movie data set since people go to movie much more during weekends. Based on various metrics, the model also shows that a MA window of about 2 weeks gives the best fit, probably because that the first two weeks of a movie's on-screen time are the most important. The change points are also reasonable: they are around Christmas and Chinese New Year.
  3. **Out of sample prediction comparison.** The *Exponential Smoothing* model predicts some unusually high values and the prediction from the ```prophet``` model is more smooth.
  4. **What does this mean for the movie industry?** It is important to pick up both weekly and annual seasonality when forecasting, as ```prophet``` is able to do. However, a simple *exponential smoothing* model with additive trend and annual seasonality does well in forecasting even though its model fit in the training set is not great.
  5. **What is next?** An easy improvement to make to the ```prophet``` model is by adding **holidays**. However, there is not an easy/straightforward way to adding Chinese holidays currently.
"""

# Download the data from my website
!wget -nc "http://www.luozijun.com/wp-content/uploads/movie_ts.csv"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv("movie_ts.csv", index_col='date', parse_dates=True)
df.head()

df.info() # There is no missing value

df.describe()

"""#Basic Information about this Data

* each column is a movie. There are multiple movies being shown in a city in a given date
* There are only three cities: Beijing (city_code: 14), Tianjin (city_code: 30), and Shanghai (city_code: 2)

## To create a time series data, I will first aggregate audience and revenue to city-date level
"""

df.loc[df['city_code'] == 2][['temp']].plot(figsize=(28, 8)) # Plot temperature and precipitation from Shanghai just for fun
df.loc[df['city_code'] == 2][['prcp']].plot(figsize=(28, 8))

df2_temp = df.loc[df['city_code'] == 2] # Keep only Shanghai
df2 = df2_temp.groupby([df2_temp.index, 'city_code'])[['audience', 'revenue']].sum() # Sum audience and revenue over all movies

df2

df2_weather = df2_temp.groupby([df2_temp.index, 'city_code'])[['temp', 'prcp']].mean() # Mean temperature and precipitation by date

df2_weather

df2 = df2.merge(df2_weather, how='left', on='date') # Merge. df2 is the final time series for Shanghai
df2.index = pd.DatetimeIndex(df2.index).to_period('D')

# Calculate percentage changes of audience and revenue
df2['aud_delta'] = df2[['audience']].pct_change(periods=1)
df2['rev_delta'] = df2[['revenue']].pct_change(periods=1)

df2

# For some reason the first day in the data set has huge measurement error
# df2.loc['2012-01-05', ['audience', 'revenue']] = np.nan
df2.loc['2012-01-06', ['aud_delta', 'rev_delta']] = np.nan

"""In the above cell,

```
df2.loc['2012-01-05', ['audience', 'revenue']] = np.nan
```
was commented out because, for reasons unknown to me, when the first date's data is dropped, ExponentialSmoothing fails to converge.



"""

df2

df['audience'].plot(figsize=(28, 8)) # Visualization

# Log audience and revenue
df2['aud_log'] = np.log(df2['audience'])
df2['rev_log'] = np.log(df2['revenue'])

df2

df2[['aud_log', 'rev_log']].plot(figsize=(28, 8))
# Since movie ticket prices are quite stable (in this data, about 60 Yuan), audience and revenue is almost perfectly collinear
# I will focus on audience for the rest of this exercise

!pip install statsmodels
from statsmodels.tsa.holtwinters import ExponentialSmoothing

ses = ExponentialSmoothing(df2['audience'], seasonal='add', seasonal_periods=365) # annual seasonality
result = ses.fit()

df2['ExponentialSmoothing'] = result.fittedvalues

df2[['ExponentialSmoothing', 'audience']].plot(figsize=(28, 8))

N_test = 200 # leaves out the last 20 days as test set
train = df2.iloc[:-N_test]
test = df2.iloc[-N_test:]

es_train = ExponentialSmoothing(train['audience'], seasonal='add', seasonal_periods=365)
res = es_train.fit()

train_idx = df2.index <= train.index[-1]
test_idx = df2.index > train.index[-1]

df2.loc[train_idx, 'ESfitted'] = res.fittedvalues
df2.loc[test_idx, 'ESfitted'] = res.forecast(N_test)
df2[['audience', 'ESfitted']].plot(figsize=(28, 8))

es_train_2 = ExponentialSmoothing(train['audience'], trend='add', seasonal='add', seasonal_periods=365)
res_2 = es_train_2.fit()

df2.loc[train_idx, 'ESfitted_Mul'] = res_2.fittedvalues
df2.loc[test_idx, 'ESfitted_Mul'] = res_2.forecast(N_test)
df2[['audience', 'ESfitted', 'ESfitted_Mul']].plot(figsize=(28, 8))

# Generate out of sample forecasts
df2_pred = pd.DataFrame(data=res_2.forecast(N_test+180)[-180:], columns=['Forecast'])
df2_pred['date'] = df2_pred.index
df2_pred
df2_new = df2.merge(df2_pred, how='outer', on='date')
df2_new.index = df2_new['date']

df2_new[['audience', 'ESfitted', 'ESfitted_Mul', 'Forecast']].plot(figsize=(28, 8))

def rmse(pred, true): # root mean squared errors
  return np.sqrt(np.mean((pred - true)**2))

def mae(pred, true): # mean absolute errors
  return np.mean(np.abs(pred - true))

print("Train RMSE (Add Seas):", rmse(train['audience'], res.fittedvalues))
print("Test RMSE (Add Trend, Add Seas):", rmse(test['audience'], res.forecast(N_test)))
print("Train RMSE (Add Seas):", rmse(train['audience'], res_2.fittedvalues))
print("Test RMSE (Add Trend, Add Seas):", rmse(test['audience'], res_2.forecast(N_test)))

print("Train MAE (Add Seas):", mae(train['audience'], res.fittedvalues))
print("Test MAE (Add Trend, Add Seas):", mae(test['audience'], res.forecast(N_test)))
print("Train MAE (Add Seas):", mae(train['audience'], res_2.fittedvalues))
print("Test MAE (Add Trend, Add Seas):", mae(test['audience'], res_2.forecast(N_test)))

"""#Facebook Prophet"""

!pip install prophet

from prophet import Prophet

# Rename according to Prophet rule
df2['ds'] = df2.index.to_timestamp()
df2['y'] = df2['audience']

df2

m = Prophet(yearly_seasonality=True, weekly_seasonality=True) # Use weekly and yearly seasonalities. Prophet automatically include these two even they are not specified explicitly
m.fit(df2)

# Predict one year ahead
future = m.make_future_dataframe(periods=365)
future.tail()

forecast = m.predict(future)

forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

fig1 = m.plot(forecast, figsize=(28, 8))

fig2 = m.plot_components(forecast)

# A different way of plotting

# from prophet.plot import plot_plotly, plot_components_plotly
# plot_plotly(m, forecast)
# plot_components_plotly(m, forecast)

from prophet.diagnostics import cross_validation, performance_metrics

df2_cv = cross_validation(
    m,
    initial='730 days',
    period='30 days',
    horizon='30 days'
)

df2_cv

pm = performance_metrics(df2_cv)
pm

from prophet.plot import plot_cross_validation_metric
plot_cross_validation_metric(df2_cv, metric='rmse');

# rmse is smallest when MA window is about 2 weeks. Probably because that's how long a movie usually is on screen

plot_cross_validation_metric(df2_cv, metric='smape');
# Quite stable in MA windows less than 14 (inclusive) days.

from prophet.plot import add_changepoints_to_plot

fig = m.plot(forecast, figsize=(28, 8))
a = add_changepoints_to_plot(fig.gca(), m, forecast)

# The above change points make sense. They are all in December (Christmas) or January/Febraury (Chinese New Year).