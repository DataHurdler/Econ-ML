# Bayesian Approaches for Randomized Controlled Trials

## Introduction

Randomized Controlled Trial (RCT) is the gold standard for establishing causality in experimental methods such as clinical trials for new drugs or field experiments in social sciences and economics. In business especially e-commerce, RCT is known as A/B/N test. The idea of RCT and A/B/N test is pretty straightforward: you have a group of individuals who are being divided randomly into groups to receive different treatments. Afterwards, the outcomes are being compared and evaluated to find out which treatment works best. In RCT, a control group, where individuals received a "placebo", is included. Placebo can be considered a type of treatment too. Note that individuals who receive a placebo are not getting "nothing". A placebo is something that has no therapeutic effect, i.e., it is not designed to cure a disease or illness. But a placebo can positively impact the wellbeing of individuals who received it, if due to nothing but psychological effect. It would be rather wrong to expect "no effect" from the controlled group that receives the placebo.

For the rest of this article, I will be using A/B/N test as the example because I want to stay away from the nitty-gritty details of RCT. We will come back to RCT toward the end. I am using "A/B/N" to include testing for more than 2 versions. If you are only comparing two versions, it is an A/B test.

When I was interviewing for a data scientist job in 2022, this was one of the interview questions: We are going to run an A/B test on a client's website. How do we determine how long we need to run the experiment? Back then I knew about how to find minimum sample size based on hypothesis testing in statistics, so I framed my answer that way. But I stopped in the middle while answering the question because something I did not think about popped into my head: how would I know the standard deviation, one of the required values to carry out the calculation for sample size, before we even run the experiment? My interview went downhill from there. Needless to say, I did not get the job. However, the interviewer was nice enough to tell me that I should look at "power analysis".

I did. And the "problem" that haunted me in the middle of my interview was confirmed. To calculate the minimum sample size required for the A/B test, we would need to know the following values: the expected effects and standard deviations of A and B, and a pre-determined statistical significance level. In fact, instead of the expected effects of A and B, all you really need is the expected difference, which can be specified. For example, suppose right now each customer who visits your website spends 50 seconds on it but you don't know that exact number right now. All you care is to increase the visit time by 15 seconds. But even in this contrived example, you still need to know the standard deviation (which means, technically speaking, you should know the expected values). How? Some suggest that you can run a short trial to estimate the standard deviation. But then, isn't the A/B test a trial itself?

After I became a data scientist, at another company, we ran an A/B test. The problem is that, according to the aforementioned power analysis, the experiment needed to be ran for at least 3 months, but we did not have that much time. After 1 month, our model (Version B) outperformed the existed model (Version A). Can we declare our model is better? According to classic A/B test design, the answer is "No" because we should not be "peeking".

The problem was, our company did not have 3 months to prove ourselves. We were in a hurry. If you think about clinical trial for new drugs, this problem is more apparent: if a drug has proved it works after the first 500 patients, yet the power analysis tells you that you need to test it on 50,000 patients, isn't it unethical to continue to give a placebo to individuals who may be benefited from the actual drug?

These two problems have bothered me for a while, until I learned about the Bayesian approaches for A/B/N testing. Here is how it works.


Alpha spending: https://online.stat.psu.edu/stat509/lesson/9/9.6